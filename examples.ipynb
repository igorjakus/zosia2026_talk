{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c55ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "\n",
    "from numba import njit\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4fbb0e",
   "metadata": {},
   "source": [
    "# 1. Vectorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b76288a",
   "metadata": {},
   "source": [
    "### 1.1 Sum of array elements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffc8d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def sum_numba(x):\n",
    "    total = 0.0\n",
    "    for i in range(x.shape[0]):\n",
    "        total += x[i]\n",
    "    return total\n",
    "\n",
    "\n",
    "def sum_numpy(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "\n",
    "@njit\n",
    "def sum_numpy_numba(x):\n",
    "    return np.sum(x)\n",
    "\n",
    "\n",
    "def sum_torch(x):\n",
    "    x_torch = torch.from_numpy(x)\n",
    "    return torch.sum(x_torch).item()\n",
    "\n",
    "\n",
    "def sum_iterative(x):\n",
    "    total = 0.0\n",
    "    for value in x:\n",
    "        total += value\n",
    "    return total\n",
    "\n",
    "\n",
    "def sum_builtin(x):\n",
    "    return sum(x)\n",
    "\n",
    "\n",
    "sum_functions = {\n",
    "    \"Numba Iterative\": sum_numba,\n",
    "    \"NumPy\": sum_numpy,\n",
    "    \"NumPy with Numba\": sum_numpy_numba,\n",
    "    \"PyTorch\": sum_torch,\n",
    "    \"Python Iterative\": sum_iterative,\n",
    "    \"Python Built-in\": sum_builtin,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e975334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(n=1_000_000, runs=10):\n",
    "    print(\"Bencharking with array size:\", n)\n",
    "    a = np.random.rand(n).astype(np.float32)\n",
    "    results = {}\n",
    "    for name, func in sum_functions.items():\n",
    "        func(a)  # warmup\n",
    "        duration = timeit.timeit(lambda: func(a), number=runs)\n",
    "        results[name] = duration / runs\n",
    "\n",
    "    for name, duration in sorted(results.items(), key=lambda item: item[1]):\n",
    "        print(f\"{name}: {duration:.6f} seconds per run\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af3ed28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bencharking with array size: 1000000\n",
      "PyTorch: 0.000049 seconds per run\n",
      "NumPy: 0.000178 seconds per run\n",
      "NumPy with Numba: 0.000783 seconds per run\n",
      "Numba Iterative: 0.000784 seconds per run\n",
      "Python Built-in: 0.077874 seconds per run\n",
      "Python Iterative: 0.109363 seconds per run\n",
      "\n",
      "Bencharking with array size: 10000000\n",
      "PyTorch: 0.001257 seconds per run\n",
      "NumPy: 0.002404 seconds per run\n",
      "Numba Iterative: 0.007988 seconds per run\n",
      "NumPy with Numba: 0.008021 seconds per run\n",
      "Python Built-in: 0.826637 seconds per run\n",
      "Python Iterative: 1.135897 seconds per run\n",
      "\n",
      "Bencharking with array size: 10000\n",
      "NumPy: 0.000007 seconds per run\n",
      "NumPy with Numba: 0.000008 seconds per run\n",
      "Numba Iterative: 0.000008 seconds per run\n",
      "PyTorch: 0.000011 seconds per run\n",
      "Python Built-in: 0.001220 seconds per run\n",
      "Python Iterative: 0.001233 seconds per run\n",
      "\n",
      "Bencharking with array size: 100\n",
      "NumPy with Numba: 0.000001 seconds per run\n",
      "Numba Iterative: 0.000001 seconds per run\n",
      "NumPy: 0.000005 seconds per run\n",
      "PyTorch: 0.000008 seconds per run\n",
      "Python Built-in: 0.000010 seconds per run\n",
      "Python Iterative: 0.000018 seconds per run\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmark(1_000_000, runs=10)\n",
    "benchmark(10_000_000, runs=10)\n",
    "benchmark(10_000, runs=10)\n",
    "benchmark(100, runs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11891d2",
   "metadata": {},
   "source": [
    "### 1.2 Sum of rows vs columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d3172",
   "metadata": {},
   "source": [
    "#### PyTorch\n",
    "\n",
    "Usually summing over rows (dim=1) is faster than summing over columns (dim=0) due to memory layout (row-major order).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36ec37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sum over array of shape: 25000x25000\n",
      "Device: cpu\n",
      "Sum Rows: 0.069855 seconds per run\n",
      "Sum Columns: 0.249759 seconds per run\n",
      "Rows faster by: 3.58x\n",
      "\n",
      "Benchmarking sum over array of shape: 10000x10000\n",
      "Device: cpu\n",
      "Sum Rows: 0.010689 seconds per run\n",
      "Sum Columns: 0.019094 seconds per run\n",
      "Rows faster by: 1.79x\n",
      "\n",
      "Benchmarking sum over array of shape: 1000x1000\n",
      "Device: cpu\n",
      "Sum Rows: 0.000029 seconds per run\n",
      "Sum Columns: 0.000033 seconds per run\n",
      "Rows faster by: 1.13x\n",
      "\n",
      "Benchmarking sum over array of shape: 100x100\n",
      "Device: cpu\n",
      "Sum Rows: 0.000005 seconds per run\n",
      "Sum Columns: 0.000005 seconds per run\n",
      "Rows faster by: 0.92x\n",
      "\n",
      "Benchmarking sum over array of shape: 100000x100\n",
      "Device: cpu\n",
      "Sum Rows: 0.001285 seconds per run\n",
      "Sum Columns: 0.002803 seconds per run\n",
      "Rows faster by: 2.18x\n",
      "\n",
      "Benchmarking sum over array of shape: 100x100000\n",
      "Device: cpu\n",
      "Sum Rows: 0.001043 seconds per run\n",
      "Sum Columns: 0.001663 seconds per run\n",
      "Rows faster by: 1.60x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark_torch(rows=1000, cols=1000, runs=10):\n",
    "    device = torch.device(\"cpu\")\n",
    "    tensor = torch.randn(rows, cols, device=device)\n",
    "    print(f\"Benchmarking sum over array of shape: {rows}x{cols}\")\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "    t_rows = timeit.timeit(lambda: torch.sum(tensor, dim=1), number=runs)\n",
    "    print(f\"Sum Rows: {t_rows / runs:.6f} seconds per run\")\n",
    "\n",
    "    t_cols = timeit.timeit(lambda: torch.sum(tensor, dim=0), number=runs)\n",
    "    print(f\"Sum Columns: {t_cols / runs:.6f} seconds per run\")\n",
    "\n",
    "    print(f\"Rows faster by: {t_cols / t_rows:.2f}x\\n\")\n",
    "\n",
    "\n",
    "runs = 50\n",
    "benchmark_torch(25_000, 25_000, runs=runs)\n",
    "benchmark_torch(10_000, 10_000, runs=runs)\n",
    "benchmark_torch(1_000, 1_000, runs=runs)\n",
    "benchmark_torch(100, 100, runs=runs)\n",
    "benchmark_torch(100_000, 100, runs=runs)\n",
    "benchmark_torch(100, 100_000, runs=runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2b7dcc",
   "metadata": {},
   "source": [
    "#### Numpy\n",
    "\n",
    "Interestingly in Numpy it doesn't work that way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8073cdaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarking sum over array of shape (25000, 25000)\n",
      "Sum Rows: 0.160197 seconds per run\n",
      "Sum Columns: 0.163368 seconds per run\n",
      "Rows faster by: 1.02x\n",
      "\n",
      "Benchmarking sum over array of shape (10000, 10000)\n",
      "Sum Rows: 0.027006 seconds per run\n",
      "Sum Columns: 0.026842 seconds per run\n",
      "Rows faster by: 0.99x\n",
      "\n",
      "Benchmarking sum over array of shape (1000, 1000)\n",
      "Sum Rows: 0.000193 seconds per run\n",
      "Sum Columns: 0.000142 seconds per run\n",
      "Rows faster by: 0.73x\n",
      "\n",
      "Benchmarking sum over array of shape (100, 100)\n",
      "Sum Rows: 0.000007 seconds per run\n",
      "Sum Columns: 0.000007 seconds per run\n",
      "Rows faster by: 1.01x\n",
      "\n",
      "Benchmarking sum over array of shape (100000, 100)\n",
      "Sum Rows: 0.003870 seconds per run\n",
      "Sum Columns: 0.004025 seconds per run\n",
      "Rows faster by: 1.04x\n",
      "\n",
      "Benchmarking sum over array of shape (100, 100000)\n",
      "Sum Rows: 0.002535 seconds per run\n",
      "Sum Columns: 0.002371 seconds per run\n",
      "Rows faster by: 0.94x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark_np(rows=1000, cols=1000, runs=10):\n",
    "    a = np.random.rand(rows, cols).astype(np.float32)\n",
    "    print(f\"Benchmarking sum over array of shape ({rows}, {cols})\")\n",
    "\n",
    "    t_rows = timeit.timeit(lambda: np.sum(a, axis=1), number=runs)\n",
    "    print(f\"Sum Rows: {t_rows / runs:.6f} seconds per run\")\n",
    "\n",
    "    t_cols = timeit.timeit(lambda: np.sum(a, axis=0), number=runs)\n",
    "    print(f\"Sum Columns: {t_cols / runs:.6f} seconds per run\")\n",
    "\n",
    "    print(f\"Rows faster by: {t_cols / t_rows:.2f}x\\n\")\n",
    "\n",
    "\n",
    "runs = 50\n",
    "benchmark_np(25_000, 25_000, runs=runs)\n",
    "benchmark_np(10_000, 10_000, runs=runs)\n",
    "benchmark_np(1_000, 1_000, runs=runs)\n",
    "benchmark_np(100, 100, runs=runs)\n",
    "benchmark_np(100_000, 100, runs=runs)\n",
    "benchmark_np(100, 100_000, runs=runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4129380c",
   "metadata": {},
   "source": [
    "# 2. Kernel Fusion\n",
    "\n",
    "Kernel is a function that runs on the GPU.\n",
    "\n",
    "Often the kernel is memory bound, meaning that the performance is limited by memory bandwidth rather than compute. That means that the kernel spends most of its time waiting for data to be loaded from memory, rather than performing computations. By fusing multiple operations into a single kernel, we can reduce the number of memory accesses, reduce overhead, and improve performance.\n",
    "\n",
    "We use GELU activation function as an example, which is defined as:\n",
    "$$\\text{GELU}(x) = x \\cdot \\frac{1}{2} \\left(1 + \\text{erf}\\left(\\frac{x}{\\sqrt{2}}\\right)\\right)$$\n",
    "\n",
    "where $\\text{erf}$ is the [error function](https://en.wikipedia.org/wiki/Error_function). This function is commonly used in deep learning models, and it can be implemented in a straightforward way using PyTorch. However, we can also fuse the operations into a single kernel using `torch.compile` to achieve better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c2751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GELU (compiled): 0.016426 seconds per run\n",
      "GELU (slow): 0.055672 seconds per run\n",
      "Compiled GELU faster by: 3.39x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.compile\n",
    "def gelu_fuse(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
    "\n",
    "\n",
    "def gelu_slow(x):\n",
    "    return x * 0.5 * (1.0 + torch.erf(x / 1.41421))\n",
    "\n",
    "\n",
    "x = torch.randn(10_000_000)\n",
    "# Warmup, torch.compile takes longer in the first iteration, as it must compile the model,\n",
    "# but in subsequent iterations, we see significant speedups compared to eager.\n",
    "\n",
    "gelu_fuse(x)\n",
    "gelu_slow(x)\n",
    "\n",
    "runs = 10\n",
    "t_gelu = timeit.timeit(lambda: gelu_fuse(x), number=runs)\n",
    "t_gelu_slow = timeit.timeit(lambda: gelu_slow(x), number=runs)\n",
    "print(f\"GELU (compiled): {t_gelu / runs:.6f} seconds per run\")\n",
    "print(f\"GELU (slow): {t_gelu_slow / runs:.6f} seconds per run\")\n",
    "print(f\"Compiled GELU faster by: {t_gelu_slow / t_gelu:.2f}x\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zosia2026-talk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
